{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model via huggingface-hub\n",
    "# !pip install huggingface-hub      \n",
    "# !huggingface-cli scan-cache \n",
    "# !huggingface-cli download TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
    "\n",
    "\n",
    "# # Run model via llama.cpp CLI\n",
    "# python -m llama_cpp.server --model models/codellama-7b.Q4_0.gguf  --n_gpu_layers 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init local model\n",
    "\n",
    "model_path = \"./models/codellama-7b.Q4_0.gguf\"\n",
    "llm = Llama(\n",
    "      model_path=model_path,\n",
    "      n_gpu_layers=-1,\n",
    "      seed=1337,\n",
    "      n_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init/load local model via huggingface\n",
    "\n",
    "repo_id, filename = \"TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF\", \"*Q2_K.gguf\"\n",
    "repo_id, filename = \"TheBloke/Llama-2-7b-Chat-GGUF\", \"*Q2_K.gguf\"\n",
    "# repo_id, filename = \"Qwen/Qwen1.5-0.5B-Chat-GGUF\", \"*q8_0.gguf\"\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=repo_id,\n",
    "    filename=filename,\n",
    "    verbose=False,\n",
    "    local_dir=\"./models/\",\n",
    "    embedding=True\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Name the names of the 2 largest planets in the solar system?\"\n",
    "# prompt = \"Return just 2 words: the names of the 2 largest planets in the solar system. No numbers no anything.\"\n",
    "# prompt = \"\"\"\n",
    "# The following code creates a graph:\n",
    "\n",
    "#     axes = df_plot.plot(kind=\"bar\", subplots=True, figsize=(14, 1.5*n_flags), legend=True, color='darkgreen')\n",
    "\n",
    "# Each subplot has a legend with the name of the parameter shown. How can I add to each legend string the counts of all values in each subplot?\n",
    "# \"\"\"\n",
    "# prompt = \"Nenne den größten Planeten des Sonnensystems\"\n",
    "# prompt = \"Wie heißt du?\"\n",
    "\n",
    "\n",
    "output = llm.create_completion(\n",
    "      f\"Q: {prompt} A: \",\n",
    "      max_tokens=100,        # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "      stop=[\"Q:\", \"\\n\"],            # Stop generating just before the model would generate a new question\n",
    "      echo=True               # Echo the prompt back in the output\n",
    ")\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant who perfectly writes poems.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a very short poem including a cat, clouds and cacao\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = llm.create_embedding(\"Hello, world!\")\n",
    "# pprint(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
